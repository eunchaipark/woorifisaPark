### **57일차 네줄 리뷰(Four Lines Review)**

- **사실(Facts)**:  
  XAI(설명 가능한 인공지능)는 AI 모델의 결정 과정을 설명하여 신뢰성과 공정성을 증진시키고, 법적 규제를 준수하는 데 도움을 줍니다. 대표적인 방법론으로 SHAP, LIME, Grad-CAM 등이 있으며, 각기 다른 방식으로 AI 모델의 예측을 설명합니다.

- **발견(Discovery)**:  
  SHAP은 협력 게임 이론을 기반으로 각 특성의 기여도를 일관성 있게 계산하고, LIME은 로컬 근사 모델을 통해 개별 예측을 설명하는 방식으로 서로 다른 상황에서의 XAI 구현을 돕습니다.

- **배운점(Lesson Learned)**:  
  XAI를 적용할 때는 설명력과 모델 성능 간의 균형이 중요하며, 도메인 특화 접근과 사용자 친화적인 인터페이스를 통해 더 명확한 설명을 제공해야 합니다. 추가적으로 Counterfactual Explanations과 Anchors 같은 개념도 유용하다는 점을 알게 되었습니다.

- **선언(Declaration)**:  
  XAI는 기술적 배경이 없는 사람에게도 신뢰성 있는 AI 모델을 설명하는 데 핵심적인 역할을 하며, 앞으로 다양한 상황에 맞춘 설명 방법을 습득하고 적용할 것입니다.

---

### **잊었다가 다시 배운 개념**

- **Grad-CAM**: CNN 모델에서 이미지 분류와 객체 탐지에 주목한 영역을 시각화하여, 모델의 예측 이유를 해석할 수 있게 해주는 기법.
  
- **Saliency Map**: 입력 데이터에서 모델이 중요하게 여긴 부분을 시각화하여, 결과에 크게 기여한 요소를 설명하는 기법.

---

### **몰랐던 개념**

- **Counterfactual Explanations**: 결과를 변화시키기 위해 입력값에서 무엇을 바꾸어야 하는지 설명하는 기법. 직관적이며 "만약 이 특성이 바뀌었다면?"이라는 질문을 해결함.
  
- **Anchors**: 개별 데이터 포인트에 대한 if-then 규칙을 통해 AI 모델의 결정을 간단한 규칙으로 설명하는 기법.
