### **59일차 네줄리뷰 (Four Lines Review)**

---

#### 59일차: 경사하강법과 파이토치 신경망 모델 학습

---

- **사실(Facts)**:  
  경사하강법을 사용하여 파이토치 모델을 학습시키고, 손실 함수는 평균 제곱 오차(MSE)를 사용해 손실을 최소화한다.

- **발견(Discovery)**:  
  파이토치에서 `nn.Linear`는 입력과 출력을 연결하는 선형 변환을 간편하게 정의할 수 있으며, Adam 옵티마이저는 학습 속도와 안정성을 높여준다.

- **배운점(Lesson Learned)**:  
  학습 과정에서 옵티마이저의 `zero_grad`, `backward`, `step` 메서드를 순차적으로 호출하는 것이 경사하강법을 효과적으로 적용하는 핵심이다.

- **선언(Declaration)**:  
  앞으로는 신경망의 구조와 하이퍼파라미터를 더 자유롭게 조정하여, 다양한 데이터셋에 맞는 모델을 설계하고 학습시키겠다.

---

### **잊었다가 다시 배운 개념**

1. **SGD 옵티마이저**:  
   확률적 경사 하강법은 전체 데이터셋을 사용하지 않고 무작위로 선택한 샘플에 대해 경사를 계산하여 빠르게 학습할 수 있다.
   
2. **파라미터 업데이트 과정**:  
   `zero_grad()`로 이전 그래디언트를 초기화하고, `backward()`로 손실에 대한 그래디언트를 계산한 후 `step()`으로 파라미터를 업데이트한다는 개념.

---

### **몰랐던 개념**

1. **자동 배치 할당**:  
   `batch_size = -1`로 설정하면 파이토치가 GPU 메모리에 맞게 배치 크기를 자동으로 설정해 준다는 점.

2. **Adam 옵티마이저의 동작 원리**:  
   Adam은 학습 속도뿐만 아니라 모멘텀과 RMSProp을 결합하여 더 빠르고 안정적인 학습을 제공한다는 점.
